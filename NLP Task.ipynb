{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tewwts = pd.read_csv('TripAdvisor Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tewwts.columns = ['num','Tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>This place is an interesting mix of really coo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Yes, we know the rooms are outdated and funky ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>We went to the Grand Canyon from Phoenix, so v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>This is my second stay at Monte Vista, it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I am not sure why folks post such negative rev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num                                             Tweets\n",
       "0    3  This place is an interesting mix of really coo...\n",
       "1    5  Yes, we know the rooms are outdated and funky ...\n",
       "2    5  We went to the Grand Canyon from Phoenix, so v...\n",
       "3    4  This is my second stay at Monte Vista, it was ...\n",
       "4    4  I am not sure why folks post such negative rev..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tewwts.head()\n",
    "# len(tewwts.Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Compro\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(tewwts)):\n",
    "    tewwts.Tweets[i] = tewwts.Tweets[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Compro\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(tewwts)):\n",
    "    tewwts.Tweets[i] = re.sub(r'\\d+','',tewwts.Tweets[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Compro\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "for i in range(0,len(tewwts)):\n",
    "    tewwts.Tweets[i] = (tewwts.Tweets[i].translate(string.maketrans(tewwts.Tweets[i],tewwts.Tweets[i]),string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>this place is an interesting mix of really coo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>yes we know the rooms are outdated and funky w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>we went to the grand canyon from phoenix so vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>this is my second stay at monte vista it was o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i am not sure why folks post such negative rev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num                                             Tweets\n",
       "0    3  this place is an interesting mix of really coo...\n",
       "1    5  yes we know the rooms are outdated and funky w...\n",
       "2    5  we went to the grand canyon from phoenix so vi...\n",
       "3    4  this is my second stay at monte vista it was o...\n",
       "4    4  i am not sure why folks post such negative rev..."
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tewwts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Compro\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(tewwts)):\n",
    "    tewwts.Tweets[i] = (tewwts.Tweets[i].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 427 * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the only thing historic about this hotel is the toilet seatswhat a waste of money what a waste of timethis hotel is run by snotty little teenagers who have no clue the thai restaurant is useless no menu in the room no menu to usemore']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = []\n",
    "for i in range(0,len(tewwts)):\n",
    "    tokenized.append(sent_tokenize(tewwts.Tweets[i]))\n",
    "#     print(tokenized)\n",
    "\n",
    "print(tokenized[426])\n",
    "len(tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words =[]\n",
    "for i in tokenized:\n",
    "    words.append(word_tokenize(str(i)))\n",
    "\n",
    "# print(len(words[426]))\n",
    "# print(words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'a',\n",
       " u'about',\n",
       " u'above',\n",
       " u'after',\n",
       " u'again',\n",
       " u'against',\n",
       " u'ain',\n",
       " u'all',\n",
       " u'am',\n",
       " u'an',\n",
       " u'and',\n",
       " u'any',\n",
       " u'are',\n",
       " u'aren',\n",
       " u\"aren't\",\n",
       " u'as',\n",
       " u'at',\n",
       " u'be',\n",
       " u'because',\n",
       " u'been',\n",
       " u'before',\n",
       " u'being',\n",
       " u'below',\n",
       " u'between',\n",
       " u'both',\n",
       " u'but',\n",
       " u'by',\n",
       " u'can',\n",
       " u'couldn',\n",
       " u\"couldn't\",\n",
       " u'd',\n",
       " u'did',\n",
       " u'didn',\n",
       " u\"didn't\",\n",
       " u'do',\n",
       " u'does',\n",
       " u'doesn',\n",
       " u\"doesn't\",\n",
       " u'doing',\n",
       " u'don',\n",
       " u\"don't\",\n",
       " u'down',\n",
       " u'during',\n",
       " u'each',\n",
       " u'few',\n",
       " u'for',\n",
       " u'from',\n",
       " u'further',\n",
       " u'had',\n",
       " u'hadn',\n",
       " u\"hadn't\",\n",
       " u'has',\n",
       " u'hasn',\n",
       " u\"hasn't\",\n",
       " u'have',\n",
       " u'haven',\n",
       " u\"haven't\",\n",
       " u'having',\n",
       " u'he',\n",
       " u'her',\n",
       " u'here',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'him',\n",
       " u'himself',\n",
       " u'his',\n",
       " u'how',\n",
       " u'i',\n",
       " u'if',\n",
       " u'in',\n",
       " u'into',\n",
       " u'is',\n",
       " u'isn',\n",
       " u\"isn't\",\n",
       " u'it',\n",
       " u\"it's\",\n",
       " u'its',\n",
       " u'itself',\n",
       " u'just',\n",
       " u'll',\n",
       " u'm',\n",
       " u'ma',\n",
       " u'me',\n",
       " u'mightn',\n",
       " u\"mightn't\",\n",
       " u'more',\n",
       " u'most',\n",
       " u'mustn',\n",
       " u\"mustn't\",\n",
       " u'my',\n",
       " u'myself',\n",
       " u'needn',\n",
       " u\"needn't\",\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'now',\n",
       " u'o',\n",
       " u'of',\n",
       " u'off',\n",
       " u'on',\n",
       " u'once',\n",
       " u'only',\n",
       " u'or',\n",
       " u'other',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'out',\n",
       " u'over',\n",
       " u'own',\n",
       " u're',\n",
       " u's',\n",
       " u'same',\n",
       " u'shan',\n",
       " u\"shan't\",\n",
       " u'she',\n",
       " u\"she's\",\n",
       " u'should',\n",
       " u\"should've\",\n",
       " u'shouldn',\n",
       " u\"shouldn't\",\n",
       " u'so',\n",
       " u'some',\n",
       " u'such',\n",
       " u't',\n",
       " u'than',\n",
       " u'that',\n",
       " u\"that'll\",\n",
       " u'the',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'them',\n",
       " u'themselves',\n",
       " u'then',\n",
       " u'there',\n",
       " u'these',\n",
       " u'they',\n",
       " u'this',\n",
       " u'those',\n",
       " u'through',\n",
       " u'to',\n",
       " u'too',\n",
       " u'under',\n",
       " u'until',\n",
       " u'up',\n",
       " u've',\n",
       " u'very',\n",
       " u'was',\n",
       " u'wasn',\n",
       " u\"wasn't\",\n",
       " u'we',\n",
       " u'were',\n",
       " u'weren',\n",
       " u\"weren't\",\n",
       " u'what',\n",
       " u'when',\n",
       " u'where',\n",
       " u'which',\n",
       " u'while',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'why',\n",
       " u'will',\n",
       " u'with',\n",
       " u'won',\n",
       " u\"won't\",\n",
       " u'wouldn',\n",
       " u\"wouldn't\",\n",
       " u'y',\n",
       " u'you',\n",
       " u\"you'd\",\n",
       " u\"you'll\",\n",
       " u\"you're\",\n",
       " u\"you've\",\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import nltk\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = []\n",
    "for i in range(0,len(tokenized)):\n",
    "    word_tokens.append(str(tokenized[i]).split(' '))\n",
    "# print(len(word_tokens))\n",
    "# print(len(word_tokens))\n",
    "# without = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "without_stopofwords = [] \n",
    "\n",
    "for i in range(0,len(word_tokens)):\n",
    "    for j in range(0,len(word_tokens[i])):\n",
    "        if(str(word_tokens[i][j]) not in stop_words):\n",
    "            without_stopofwords.append(word_tokens[i][j])\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "# print(filtered_sentence[5])        \n",
    "    \n",
    "  \n",
    "# for w in range(len(word_tokens)):\n",
    "#     if str(word_tokens[w][10]) not in stop_words:\n",
    "#         filtered_sentence.append(word_tokens[w]) \n",
    "# #         print(filtered_sentence[w])\n",
    "#     else:\n",
    "#         continue\n",
    "        \n",
    "        #         filtered_sentence = ' '.join(filtered_sentence)\n",
    "#         print(filtered_sentence)\n",
    "              \n",
    "# print(filtered_sentence[0][10])\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_word_list = word_tokens[:] #make a copy of the word_list\n",
    "# for word in word_list: # iterate over word_list\n",
    "#   if word in stopwords.words('english'): \n",
    "#     filtered_word_list.remove(word) # remove word from filtered_word_list if it is a stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(tokenized)):\n",
    "#     review = re.sub('[^a-zA-Z]', ' ', tokenized[i])\n",
    "#     review = review.lower()\n",
    "#     review = review.split()\n",
    "#     review = [word for word in review if not word in stop_words]\n",
    "#     review = ' '.join(review)\n",
    "#     corpus.append(review)\n",
    "#     print(corpus[i])\n",
    "# sir this unhashable list error is bcz of python 2 kernal it works finr in python 3 as i have tested i'll attache python 3 file as well actually mu notebook kernal is of python 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer= PorterStemmer()\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without_stopofwords[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(tokenized)):\n",
    "#     word = nltk.word_tokenize(str(tokenized[i]))\n",
    "stem_words = []\n",
    "stem_words = [stemmer.stem(w) for w in without_stopofwords]\n",
    "\n",
    "#     tokenized[i] = ' '.join(word)\n",
    "#     print(tokenized[i])\n",
    "# stem_words[1]\n",
    "# stem_words[12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lematizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "leem_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(without_stopofwords)):\n",
    "# sen = []\n",
    "# leem_words = [lematizer.lemmatize(w) for w in without_stopofwords]\n",
    "#  sen.appe' '.join(leem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmtzr = WordNetLemmatizer()\n",
    "# tokens = [lematizer.lemmatize(word) for word in without_stopofwords]\n",
    "# preprocessed_text= ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(tokenized)):\n",
    "#     leem_words = [w for w in without_stopofwords]\n",
    "#     leem_words = ' '.join(leem_words)\n",
    "# len(sen)\n",
    "# sen[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'really'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leem_words[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10646"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(427, 2385)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_values = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11950448, 0.18965788, 0.25981129, ..., 0.23441059, 0.23441059,\n",
       "       0.23441059])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_values.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "for i in range(len(tokenized)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(tokenized[i]))\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lematizer.lemmatize(word) for word in review if not word in stop_words]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "len(corpus)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = []\n",
    "# for i in range(len(tokenized)):\n",
    "#     for j in range(len(tokenized[i])):\n",
    "#         review = re.sub('[^a-zA-Z]', ' ', str(tokenized[j]))\n",
    "# #     review = review.lower()\n",
    "# #     review = review.split()\n",
    "#         review = str(leem_words[j]) + str(\" \")+str(leem_words[j+len(tokenized[j+1])])\n",
    "#         corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tewwts.Tweets[1]\n",
    "# for i in range(len(corpus)):\n",
    "    \n",
    "#     print(corpus[i])\n",
    "# corpus[10645]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(leem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
